import yaml
import json
import argparse
from tqdm import tqdm
import numpy as np
import faiss
from pathlib import Path
import os
import logging
from sentence_transformers import SentenceTransformer
import sys

# --- æ—¥å¿—è®¾ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°sys.path
sys.path.append(str(Path(__file__).resolve().parents[1]))

from ksf.utils.data_utils import load_knowledge_base_from_file

class UnifiedIndexBuilder:
    """
    æ„å»ºå¹¶ä¿å­˜ç»Ÿä¸€çš„FAISSç´¢å¼•åŠå…¶å…ƒæ•°æ®ã€‚
    è¯¥ç´¢å¼•å¯ä»¥å¹¶è½¨èåˆä¸‰ç§çŸ¥è¯†æºï¼š
    1. ä¸»çŸ¥è¯† (Primary Knowledge): æ ¸å¿ƒäº‹å®ï¼Œæ ‡è®°ä¸º 'primary_atom'ã€‚
    2. ä¸Šä¸‹æ–‡çŸ¥è¯† (Contextual Knowledge): èƒŒæ™¯ã€åŸåˆ™ã€æ³¨è§£ï¼Œæ ‡è®°ä¸º 'context_atom'ã€‚
    3. æ¦‚å¿µè¯æ±‡ (Concept Vocabulary): ç”¨äºæ„å›¾å‘ç°çš„å…³é”®è¯ï¼Œæ ‡è®°ä¸º 'concept'ã€‚
    """
    def __init__(self, config_path: str):
        logger.info(f"æ­£åœ¨ä» {config_path} åŠ è½½é…ç½®...")
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        discoverer_config = self.config['discoverer']
        self.model_name = discoverer_config['model_name']
        self.adapter_path = discoverer_config.get('adapter_path')
        self.index_dir = Path(discoverer_config['index_dir'])
        
        self.model = self._load_model()
        logger.info("ç»Ÿä¸€ç´¢å¼•æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆã€‚")

    def _load_model(self) -> SentenceTransformer:
        logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
        model = SentenceTransformer(self.model_name)
        if self.adapter_path and os.path.exists(self.adapter_path):
            try:
                model.load_adapter(self.adapter_path)
                logger.info(f"âœ“ æˆåŠŸä» {self.adapter_path} åŠ è½½PEFTé€‚é…å™¨ã€‚")
            except Exception as e:
                logger.error(f"åŠ è½½é€‚é…å™¨å¤±è´¥: {e}")
        return model

    def build(self, primary_path, context_path=None, vocabulary_path=None):
        """æ‰§è¡Œå®Œæ•´çš„æ„å»ºæµç¨‹ã€‚"""
        logger.info("ğŸš€ å¼€å§‹æ„å»ºç»Ÿä¸€ç´¢å¼•...")

        # 1. åŠ è½½æ‰€æœ‰æ•°æ®æº
        logger.info("--- æ­¥éª¤ 1/4: åŠ è½½æ‰€æœ‰æ•°æ®æº ---")
        primary_data, context_data, vocab_data = self._load_all_sources(
            primary_path, context_path, vocabulary_path
        )

        # 2. ç¼–ç æ‰€æœ‰å†…å®¹
        logger.info("--- æ­¥éª¤ 2/4: ç¼–ç æ‰€æœ‰æ–‡æœ¬ä¸ºå‘é‡ ---")
        all_texts = ([item['content'] for item in primary_data] +
                     [item['content'] for item in context_data] +
                     vocab_data)
        
        if not all_texts:
            logger.error("é”™è¯¯: æ²¡æœ‰åŠ è½½ä»»ä½•æ•°æ®ï¼Œæ— æ³•æ„å»ºç´¢å¼•ã€‚")
            return
            
        embeddings = self.model.encode(
            all_texts, 
            normalize_embeddings=True, 
            convert_to_numpy=True,
            show_progress_bar=True
        )
        logger.info(f"âœ“ æ‰€æœ‰ {len(all_texts)} ä¸ªæ¡ç›®ç¼–ç å®Œæˆã€‚å‘é‡ç»´åº¦: {embeddings.shape[1]}")
        
        # 3. æ„å»ºå¹¶ä¿å­˜FAISSç´¢å¼•
        logger.info("--- æ­¥éª¤ 3/4: æ„å»ºå¹¶ä¿å­˜FAISSç´¢å¼• ---")
        index = self._build_faiss_index(embeddings)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        index_path = self.index_dir / "faiss.index"
        faiss.write_index(index, str(index_path))
        logger.info(f"âœ“ FAISSç´¢å¼•å·²æ„å»ºå¹¶ä¿å­˜è‡³ {index_path}")

        # 4. æ„å»ºå¹¶ä¿å­˜å…ƒæ•°æ®
        logger.info("--- æ­¥éª¤ 4/4: æ„å»ºå¹¶ä¿å­˜ç´¢å¼•å…ƒæ•°æ® ---")
        index_meta = self._build_index_meta(primary_data, context_data, vocab_data)
        meta_path = self.index_dir / "index_meta.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(index_meta, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ“ ç´¢å¼•å…ƒæ•°æ®å·²æ„å»ºå¹¶ä¿å­˜è‡³ {meta_path}")

        logger.info("âœ… ç»Ÿä¸€ç´¢å¼•æ„å»ºæµç¨‹å…¨éƒ¨å®Œæˆï¼")

    def _load_all_sources(self, primary_path, context_path, vocabulary_path):
        """åŠ è½½æ‰€æœ‰æä¾›çš„çŸ¥è¯†æºæ–‡ä»¶ã€‚"""
        primary_data = load_knowledge_base_from_file(primary_path)
        logger.info(f"å·²åŠ è½½ {len(primary_data)} æ¡ä¸»çŸ¥è¯†ã€‚")

        context_data = []
        if context_path:
            context_data = load_knowledge_base_from_file(context_path)
            logger.info(f"å·²åŠ è½½ {len(context_data)} æ¡ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚")

        vocab_data = []
        if vocabulary_path:
            try:
                with open(vocabulary_path, 'r', encoding='utf-8') as f:
                    vocab_data = [line.strip() for line in f if line.strip()]
                logger.info(f"å·²åŠ è½½ {len(vocab_data)} ä¸ªæ¦‚å¿µè¯æ±‡ã€‚")
            except FileNotFoundError:
                logger.warning(f"è­¦å‘Š: æœªæ‰¾åˆ°è¯æ±‡è¡¨æ–‡ä»¶ {vocabulary_path}ï¼Œå°†å¿½ç•¥ã€‚")

        return primary_data, context_data, vocab_data

    def _build_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:
        dimension = embeddings.shape[1]
        # ä½¿ç”¨IndexFlatIPï¼Œå› ä¸ºæˆ‘ä»¬å¤„ç†çš„æ˜¯å½’ä¸€åŒ–çš„åµŒå…¥ï¼Œå†…ç§¯ç­‰äºä½™å¼¦ç›¸ä¼¼åº¦
        index = faiss.IndexFlatIP(dimension)
        index.add(embeddings.astype('float32'))
        return index

    def _build_index_meta(self, primary_data, context_data, vocab_data):
        """ä¸ºæ‰€æœ‰æºæ„å»ºç»Ÿä¸€çš„å…ƒæ•°æ®åˆ—è¡¨ï¼Œå¹¶æ‰“ä¸Šæ­£ç¡®çš„ç±»å‹æ ‡ç­¾ã€‚"""
        meta = []
        current_index = 0

        # æ·»åŠ ä¸»çŸ¥è¯†å…ƒæ•°æ®
        for item in primary_data:
            meta.append({
                "index_id": current_index,
                "type": "primary_atom",
                "id": item.get('id', f'primary_{current_index}'),
                "content": item['content']
            })
            current_index += 1
        
        # æ·»åŠ ä¸Šä¸‹æ–‡çŸ¥è¯†å…ƒæ•°æ®
        for item in context_data:
            meta.append({
                "index_id": current_index,
                "type": "context_atom",
                "id": item.get('id', f'context_{current_index}'),
                "content": item['content']
            })
            current_index += 1

        # æ·»åŠ æ¦‚å¿µè¯æ±‡å…ƒæ•°æ®
        for word in vocab_data:
            meta.append({
                "index_id": current_index,
                "type": "concept",
                "id": f"concept_{current_index}",
                "content": word
            })
            current_index += 1
            
        return meta

def main():
    parser = argparse.ArgumentParser(description="æ„å»ºç»Ÿä¸€çš„ã€å¯å¹¶è½¨å¤šç§çŸ¥è¯†æºçš„FAISSç´¢å¼•ã€‚")
    parser.add_argument(
        '--config', 
        type=str, 
        default="configs/ksf_config.yaml", 
        help="KSFé…ç½®æ–‡ä»¶è·¯å¾„ã€‚"
    )
    parser.add_argument(
        '--primary', 
        type=str, 
        required=True, 
        help="ä¸»çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„ (.jsonlæ ¼å¼)ã€‚"
    )
    parser.add_argument(
        '--context', 
        type=str, 
        default=None, 
        help="(å¯é€‰) ä¸Šä¸‹æ–‡çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„ (.jsonlæ ¼å¼)ã€‚"
    )
    parser.add_argument(
        '--vocabulary', 
        type=str, 
        default=None, 
        help="(å¯é€‰) æ¦‚å¿µè¯æ±‡æ–‡ä»¶è·¯å¾„ (.txtæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªè¯)ã€‚"
    )
    args = parser.parse_args()

    if not os.path.exists(args.config):
        logger.error(f"è‡´å‘½é”™è¯¯: é…ç½®æ–‡ä»¶ '{args.config}' ä¸å­˜åœ¨ã€‚")
        return
    
    builder = UnifiedIndexBuilder(config_path=args.config)
    builder.build(
        primary_path=args.primary,
        context_path=args.context,
        vocabulary_path=args.vocabulary
    )

if __name__ == "__main__":
    main() 