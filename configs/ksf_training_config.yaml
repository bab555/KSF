# KSF V2 Training Configuration
# This file centralizes all parameters for the KSF V2 model training,
# covering model architecture, data paths, training settings, and logging.

# --- Base Model Configuration ---
# Defines the core transformer model used as the foundation for KSF.
base_model:
  path: "../qwen3" # Local path to Qwen3 model
  hidden_size: 2560             # Core dimension of the transformer model (e.g., Qwen3-4B)
  vocab_size: 151669             # Vocabulary size, specific to the tokenizer (corrected from 151851)
  torch_dtype: "bfloat16"       # Data type for training (bfloat16 is recommended for modern GPUs)

# --- Model-Specific Configurations ---
# Contains parameters for the custom modules within the KSF architecture.
model:
  # Bypass valve configuration for special generation modes
  generate_query_token_id: 151670 # Token ID for triggering special generation modes (e.g., keyword extraction)
  
  # Specific parameters for the AdvancedSynthesizer module (S-Module)
  advanced_synthesizer:
    num_heads: 8 # Number of attention heads for all attention layers in the S-Module
    num_decoder_layers: 2 # Number of layers for the final deep-fusion Transformer Decoder

  # Specific parameters for the KnowledgeBank module (K-Module)
  knowledge_bank:
    # Inherits hidden_size from the base model config
    memory_matrix_size: 16384 # Number of knowledge vectors in the bank
    
    # Configuration for the Quantum Knowledge Fluid platforms
    platforms:
      L3: # High-level abstract concepts platform
        size: 1024 # Number of concept vectors
      L2: # Mid-level semantic platform  
        size: 2048 # Number of semantic vectors
      L1: # Low-level concrete platform
        size: 4096 # Number of concrete knowledge vectors
    
    # Configuration for the cascading projection system
    projectors:
      project_to_L3: # Initial query -> L3 projection
        heads: 8
      project_to_L2: # L3 -> L2 projection
        heads: 8  
      project_to_L1: # L2 -> L1 projection
        heads: 8
    
    # Configuration for the thinking module
    thinker:
      enabled: true # Enable deep thinking self-attention
      heads: 8 # Number of attention heads
      layers: 2 # Number of transformer encoder layers
    
    # Configuration for the knowledge injection mechanism
    knowledge_injection:
      enabled: true # Enable/disable the knowledge injection functionality
      embedding_model_id: "../qwen3-embedding/Qwen/Qwen3-Embedding-4B" # Local path to Qwen3-Embedding model
      embedding_dimension: 2560 # Dimension of the pre-trained embedding model
      requires_adapter: false # No adapter needed as dimensions match base model hidden_size

# --- Data Configuration ---
# Specifies file paths for training, evaluation, and the knowledge base.
data:
  train_file: "data/staged_training_test/train/ksf_train.jsonl"
  eval_file: "data/staged_training_test/eval/ksf_eval.jsonl"
  knowledge_base_file: "data/knowledge_base.txt"

# --- Training Configuration ---
# Parameters that control the training loop and optimization process.
training:
  # General training settings from transformers.TrainingArguments
  output_dir: "./checkpoints/ksf_v2"
  overwrite_output_dir: true
  num_train_epochs: 1
  per_device_train_batch_size: 1 # Adjust based on GPU memory
  gradient_accumulation_steps: 8 # Effective batch size = train_batch_size * grad_accum * num_gpus
  save_strategy: "steps"
  save_steps: 5
  logging_strategy: "steps"
  logging_steps: 5
  learning_rate: 2.0e-5
  weight_decay: 0.0
  fp16: false # Set to true if your GPU supports it
  
  # Optimizer settings
  optimizer:
    type: "AdamW"
    betas: [0.9, 0.999]
    eps: 1.0e-8
  
  # Scheduler settings
  lr_scheduler:
    type: "cosine"
    warmup_ratio: 0.03
  
  # Batch settings
  batch_size: 1 # Adjust based on GPU memory

# --- Loss Configuration ---
# Configuration for the loss function
loss:
  main_weight: 1.0 # Weight for the primary cross-entropy task loss
  summary_loss:
    enabled: true # Whether to use the auxiliary summary loss
    weight: 0.1   # The weight for the summary loss term

# --- Logging Configuration ---
# Settings for logging metrics and training progress.
logging:
  log_to: "wandb" # or "tensorboard", "file"
  project_name: "KSF_V2_Training"
  run_name: "Qwen3-4B-Base-Initial-Run"
  log_level: "INFO"
  log_interval: 10 