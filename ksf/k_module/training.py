"""
Kæ¨¡å—è®­ç»ƒä»£ç 
æ•´åˆå¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå’Œé¢†åŸŸé€‚é…å¾®è°ƒ
"""

import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from pathlib import Path
import logging
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ContrastiveLearningDataset(Dataset):
    """
    å¯¹æ¯”å­¦ä¹ æ•°æ®é›†
    ç”¨äºKæ¨¡å—çš„é¢„è®­ç»ƒï¼Œæ„å»ºé€šç”¨çš„è¯­ä¹‰åœ°å›¾
    """
    
    def __init__(self, data_path: str, max_samples: Optional[int] = None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        """
        self.data = self._load_data(data_path)
        if max_samples:
            self.data = self.data[:max_samples]
    
    def _load_data(self, data_path: str) -> List[InputExample]:
        """
        åŠ è½½è®­ç»ƒæ•°æ®
        æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
        """
        examples = []
        
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                for i, item in enumerate(data):
                    if isinstance(item, dict):
                        # å¤„ç†é—®ç­”å¯¹æ ¼å¼
                        if 'question' in item and 'answer' in item:
                            examples.append(InputExample(
                                texts=[item['question'], item['answer']],
                                label=1.0  # æ­£æ ·æœ¬
                            ))
                        # å¤„ç†æŸ¥è¯¢-çŸ¥è¯†å¯¹æ ¼å¼
                        elif 'query' in item and 'knowledge' in item:
                            examples.append(InputExample(
                                texts=[item['query'], item['knowledge']],
                                label=1.0
                            ))
                        # å¤„ç†å†…å®¹å¯¹æ ¼å¼
                        elif 'text1' in item and 'text2' in item:
                            label = item.get('label', 1.0)
                            examples.append(InputExample(
                                texts=[item['text1'], item['text2']],
                                label=float(label)
                            ))
            
            logger.info(f"âœ“ åŠ è½½äº† {len(examples)} ä¸ªè®­ç»ƒæ ·æœ¬")
            
        except Exception as e:
            logger.error(f"âœ— åŠ è½½æ•°æ®å¤±è´¥: {e}")
            examples = []
        
        return examples
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]


class KModuleTrainer:
    """
    Kæ¨¡å—è®­ç»ƒå™¨
    è´Ÿè´£é¢„è®­ç»ƒå’Œå¾®è°ƒ
    """
    
    def __init__(self, 
                 model_name: str = "all-mpnet-base-v2",
                 output_dir: str = "./checkpoints/k_module",
                 max_seq_length: int = 512):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model_name: åŸºç¡€æ¨¡å‹åç§°
            output_dir: è¾“å‡ºç›®å½•
            max_seq_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.model_name = model_name
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.max_seq_length = max_seq_length
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = SentenceTransformer(model_name)
        self.model.max_seq_length = max_seq_length
        
        logger.info(f"âœ“ åˆå§‹åŒ–Kæ¨¡å—è®­ç»ƒå™¨ï¼ŒåŸºç¡€æ¨¡å‹: {model_name}")
    
    def pretrain_contrastive(self, 
                           train_data_path: str,
                           eval_data_path: Optional[str] = None,
                           batch_size: int = 16,
                           epochs: int = 3,
                           learning_rate: float = 2e-5,
                           warmup_steps: int = 1000,
                           max_samples: Optional[int] = None):
        """
        å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ
        æ„å»ºé€šç”¨çš„è¯­ä¹‰åœ°å›¾
        
        Args:
            train_data_path: è®­ç»ƒæ•°æ®è·¯å¾„
            eval_data_path: è¯„ä¼°æ•°æ®è·¯å¾„
            batch_size: æ‰¹æ¬¡å¤§å°
            epochs: è®­ç»ƒè½®æ•°
            learning_rate: å­¦ä¹ ç‡
            warmup_steps: é¢„çƒ­æ­¥æ•°
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        """
        logger.info("ğŸš€ å¼€å§‹å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ...")
        
        # åŠ è½½æ•°æ®
        train_dataset = ContrastiveLearningDataset(train_data_path, max_samples)
        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        train_loss = losses.CosineSimilarityLoss(self.model)
        
        # è®¾ç½®è¯„ä¼°å™¨
        evaluator = None
        if eval_data_path and Path(eval_data_path).exists():
            eval_dataset = ContrastiveLearningDataset(eval_data_path, max_samples)
            if len(eval_dataset) > 0:
                # æ„å»ºè¯„ä¼°æ ·æœ¬
                eval_examples = []
                for example in eval_dataset.data[:100]:  # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°é‡
                    eval_examples.append(InputExample(
                        texts=example.texts,
                        label=example.label
                    ))
                
                evaluator = EmbeddingSimilarityEvaluator.from_input_examples(
                    eval_examples, 
                    name='eval'
                )
        
        # è®­ç»ƒ
        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            evaluator=evaluator,
            epochs=epochs,
            evaluation_steps=1000,
            warmup_steps=warmup_steps,
            output_path=str(self.output_dir / "pretrained"),
            save_best_model=True,
            optimizer_params={'lr': learning_rate}
        )
        
        logger.info("âœ“ å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå®Œæˆ")
    
    def finetune_domain_adaptation(self,
                                 knowledge_base_path: str,
                                 batch_size: int = 8,
                                 epochs: int = 2,
                                 learning_rate: float = 1e-5):
        """
        é¢†åŸŸé€‚é…å¾®è°ƒ
        å¼ºåŒ–ç‰¹å®šé¢†åŸŸçŸ¥è¯†åœ¨è¯­ä¹‰åœ°å›¾ä¸­çš„è¿æ¥æƒé‡
        
        Args:
            knowledge_base_path: é¢†åŸŸçŸ¥è¯†åº“è·¯å¾„
            batch_size: æ‰¹æ¬¡å¤§å°
            epochs: è®­ç»ƒè½®æ•°
            learning_rate: å­¦ä¹ ç‡
        """
        logger.info("ğŸ¯ å¼€å§‹é¢†åŸŸé€‚é…å¾®è°ƒ...")
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        pretrained_path = self.output_dir / "pretrained"
        if pretrained_path.exists():
            self.model = SentenceTransformer(str(pretrained_path))
            logger.info(f"âœ“ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrained_path}")
        else:
            logger.warning("âš ï¸ é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒ")
        
        # ä»çŸ¥è¯†åº“æ„å»ºè®­ç»ƒæ•°æ®
        train_examples = self._create_domain_training_data(knowledge_base_path)
        
        if not train_examples:
            logger.error("âœ— æ— æ³•ä»çŸ¥è¯†åº“åˆ›å»ºè®­ç»ƒæ•°æ®")
            return
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        train_loss = losses.MultipleNegativesRankingLoss(self.model)
        
        # å¾®è°ƒ
        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=epochs,
            warmup_steps=100,
            output_path=str(self.output_dir / "domain_adapted"),
            save_best_model=True,
            optimizer_params={'lr': learning_rate}
        )
        
        logger.info("âœ“ é¢†åŸŸé€‚é…å¾®è°ƒå®Œæˆ")
    
    def _create_domain_training_data(self, knowledge_base_path: str) -> List[InputExample]:
        """
        ä»çŸ¥è¯†åº“åˆ›å»ºé¢†åŸŸè®­ç»ƒæ•°æ®
        ç”ŸæˆçŸ¥è¯†é¡¹ä¹‹é—´çš„ç›¸å…³æ€§å¯¹
        """
        examples = []
        
        try:
            with open(knowledge_base_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å¤„ç†ä¸åŒæ ¼å¼çš„çŸ¥è¯†åº“
            knowledge_items = []
            if isinstance(data, list):
                knowledge_items = data
            elif isinstance(data, dict):
                if 'data' in data:
                    knowledge_items = data['data']
                elif 'knowledge' in data:
                    knowledge_items = data['knowledge']
            
            # ä¸ºæ¯ä¸ªçŸ¥è¯†é¡¹åˆ›å»ºè®­ç»ƒæ ·æœ¬
            for item in knowledge_items:
                if isinstance(item, dict):
                    # å¦‚æœæœ‰é—®ç­”å¯¹ï¼Œåˆ›å»ºæ­£æ ·æœ¬
                    if 'question' in item and 'answer' in item:
                        examples.append(InputExample(
                            texts=[item['question'], item['answer']],
                            label=1.0
                        ))
                    
                    # å¦‚æœæœ‰å†…å®¹å­—æ®µï¼Œåˆ›å»ºè‡ªç›¸å…³æ ·æœ¬
                    content = item.get('content') or item.get('text', '')
                    if content:
                        # å°†å†…å®¹åˆ†å‰²æˆå¥å­ï¼Œåˆ›å»ºå¥å­é—´çš„ç›¸å…³æ€§
                        sentences = content.split('ã€‚')
                        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
                        
                        # åˆ›å»ºç›¸é‚»å¥å­çš„æ­£æ ·æœ¬å¯¹
                        for i in range(len(sentences) - 1):
                            examples.append(InputExample(
                                texts=[sentences[i], sentences[i + 1]],
                                label=0.8  # ä¸­ç­‰ç›¸å…³æ€§
                            ))
            
            logger.info(f"âœ“ ä»çŸ¥è¯†åº“åˆ›å»ºäº† {len(examples)} ä¸ªè®­ç»ƒæ ·æœ¬")
            
        except Exception as e:
            logger.error(f"âœ— åˆ›å»ºé¢†åŸŸè®­ç»ƒæ•°æ®å¤±è´¥: {e}")
        
        return examples
    
    def save_model(self, save_path: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        self.model.save(save_path)
        logger.info(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        self.model = SentenceTransformer(model_path)
        logger.info(f"âœ“ æ¨¡å‹å·²åŠ è½½: {model_path}")


def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Kæ¨¡å—è®­ç»ƒå™¨
    """
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = KModuleTrainer(
        model_name="all-mpnet-base-v2",
        output_dir="./checkpoints/k_module"
    )
    
    # é˜¶æ®µ1: å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é€šç”¨çš„å¯¹æ¯”å­¦ä¹ æ•°æ®
    # trainer.pretrain_contrastive(
    #     train_data_path="./data/contrastive_train.json",
    #     eval_data_path="./data/contrastive_eval.json",
    #     batch_size=16,
    #     epochs=3
    # )
    
    # é˜¶æ®µ2: é¢†åŸŸé€‚é…å¾®è°ƒ
    trainer.finetune_domain_adaptation(
        knowledge_base_path="./data/äº‘å’Œæ–‡æ—…çŸ¥è¯†åº“æ•°æ®é›†.json",
        batch_size=8,
        epochs=2
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_model("./checkpoints/k_module/final")


if __name__ == "__main__":
    main() 